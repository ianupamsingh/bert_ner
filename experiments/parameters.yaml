task:
  model:
    encoder:
      type: bert
      bert:
        attention_dropout_rate: 0.1
        dropout_rate: 0.1
        hidden_activation: gelu
        hidden_size: 768
        initializer_range: 0.02
        intermediate_size: 3072
        max_position_embeddings: 512
        num_attention_heads: 12
        num_layers: 12
        type_vocab_size: 2
        vocab_size: 30522
  train_data:
    input_path: '/home/anupam/ModelExperiments/Data/preprocessed/train.tfrecord'
    is_training: true
    global_batch_size: 4
  validation_data:
    input_path: '/home/anupam/ModelExperiments/Data/preprocessed/dev.tfrecord'
    is_training: false
    global_batch_size: 4
trainer:
  checkpoint_interval: 50
  max_to_keep: 5
  steps_per_loop: 10
  summary_interval: 10
  train_steps: 500
  validation_interval: 10
  validation_steps: -1
  optimizer_config:
    learning_rate:
      polynomial:
        initial_learning_rate: 0.001
        decay_steps: 740000
    warmup:
      polynomial:
        warmup_steps: 10
